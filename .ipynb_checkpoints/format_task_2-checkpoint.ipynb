{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name:\n",
    "#### Student ID: \n",
    "\n",
    "Date: 02/04/2017\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* collections (for calculation frequency )\n",
    "* re 2.2.1 (for regular expression) \n",
    "* os (for join path, split file name, check the file if exists)\n",
    "\n",
    "## 1. Introduction\n",
    "his task is to build sparse representations for the meeting transcripts generated in task 1, which includes word tokenization, vocabulary generation, and the generation of sparse representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join, split, exists, splitext\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word tokenization, vocabulary generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure stop word list. \n",
    "Loop reading the txt file generated by task 1 for word segmentation, deleting stop words, and summing up all words.\n",
    "Calculate the frequency of words and delete words that exceed the frequency of 132.\n",
    "the vocabulary was sorted in alphabetic order，add an index，save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read stop words from ./stopwords_en.txt file to construct stop word list\n",
    "stopwords_file = './stopwords_en.txt'\n",
    "f = open(stopwords_file)\n",
    "line = f.readline()\n",
    "stopwords = []\n",
    "while line:\n",
    "    stopwords.append(line.strip())\n",
    "    line = f.readline()\n",
    "    \n",
    "# Define a word segmentation function, enter a segment, use regular expression word segmentation, and remove stop words\n",
    "def extract_tokens(document, stopwords):\n",
    "    # Implement word segmentation using regular expression matching, and turn the word into lowercase\n",
    "    words = re.findall(\"\\w+(?:[-']\\w+)?\", document.lower())\n",
    "    # remove word from stop word list\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "# read the txt file generated by task 1 and generate a list with each sentence as an element\n",
    "def load_txt(txt_file):\n",
    "    f = open(txt_file)\n",
    "    line = f.readline()\n",
    "    sentence_list = []\n",
    "    while line: \n",
    "        # remove asterisk separator\n",
    "        if line.strip() != '**********':\n",
    "            sentence_list.append(line.strip())\n",
    "        line = f.readline()\n",
    "    return sentence_list\n",
    "\n",
    "# Call load_txt, read multiple txt files, generate a list of elements inside\n",
    "def batch_load_txt(onlyfiles):\n",
    "    meeting_transcript_list = []\n",
    "    for txt_file in onlyfiles: \n",
    "        # call load_txt function to read a txt file\n",
    "        meeting_transcript = load_txt(txt_file)\n",
    "        # fatten byextend\n",
    "        meeting_transcript_list.extend(meeting_transcript)\n",
    "    return meeting_transcript_list\n",
    "\n",
    "# input a list of multiple paragraphs to segment each paragraph in the list\n",
    "def generate_segment_tokens_list(meeting_transcript_list):\n",
    "    # segmentation of each paragraph, a list of words after a paragraph, as a sub-list exists in a list\n",
    "    meeting_transcript_list_tokens = list(map(lambda x: extract_tokens(x, stopwords), meeting_transcript_list))\n",
    "    # initialize a dict with a default value of 0\n",
    "    frequency = defaultdict(int)\n",
    "    # Each sublist in the circular list meeting_transcript_list_tokens\n",
    "    for meeting_transcript_tokens in meeting_transcript_list_tokens:\n",
    "        for token in meeting_transcript_tokens:\n",
    "            # calculate the frequency of words\n",
    "            frequency[token] += 1\n",
    "    # Use the frequency calculation results to delete words with a frequency greater than 132 in meeting_transcript_list_tokens     \n",
    "    tokens_list = [[token for token in meeting_transcript_tokens if frequency[token] <= 132] for meeting_transcript_tokens in meeting_transcript_list_tokens]\n",
    "    # return filter result\n",
    "    return tokens_list\n",
    "\n",
    "# Use a list containing multiple paragraphs to generate a dictionary of words, key is a word, and value is an index\n",
    "# Words in the vocabulary was sorted in alphabetic order\n",
    "# The tokens_list input is the output of generate_segment_tokens_list\n",
    "def generate_token_dict(tokens_list):\n",
    "    tokens_set = []\n",
    "    #  Take out the words from each sub-list into a large list\n",
    "    [tokens_set.extend(tokens) for tokens in tokens_list]\n",
    "    # Drop duplicates words and sort\n",
    "    distinct_sorted_token = sorted(set(tokens_set))\n",
    "    # Create the index of word\n",
    "    token_idx = range(0, len(distinct_sorted_token))\n",
    "    # The word index and word are combined as a key-value pair, the key is word, and the value is index\n",
    "    sorted_token_set = zip(distinct_sorted_token, token_idx)\n",
    "    # Put key-value pairs into a dict\n",
    "    token_dict = dict(sorted_token_set)\n",
    "    return token_dict\n",
    "\n",
    "# Save word and word index to specified file\n",
    "# Input a key is a vocabulary, index is a dict of value, specify the output file\n",
    "def output_vocab(vocab_dict, output_file):\n",
    "    # Vocab_dict turns into list and sorts\n",
    "    vocab_list = [(word, idx) for word, idx in vocab_dict.items()]\n",
    "    vocab_list_sorted = sorted(vocab_list, key=lambda x:x[0])\n",
    "    f = open(output_file, 'w')\n",
    "    for word, idx in vocab_list_sorted:\n",
    "        f.write(\"%s:%s\\n\" % (word,idx))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_files_dir = './txt_files'\n",
    "onlyfiles = [join(txt_files_dir, f) for f in listdir(txt_files_dir) if (isfile(join(txt_files_dir, f))) and (splitext(f)[1] == '.txt')]\n",
    "\n",
    "meeting_transcript_list = batch_load_txt(onlyfiles)\n",
    "segment_tokens_list = generate_segment_tokens_list(meeting_transcript_list)\n",
    "token_dict = generate_token_dict(segment_tokens_list)\n",
    "\n",
    "vocab_output_file = './vocab.txt'\n",
    "output_vocab(token_dict, vocab_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate the topic boundaries encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle through the task1 files generated by each task1 and do the following:  \n",
    "1) Calculate the number of lines in the file   \n",
    "2) Record the position of the separator   \n",
    "3) Generate a vector of 0 length equal to the number of rows. Assign a value of 1 to the previous element at the separator   position, indicating the topic boundary  \n",
    "4) Vector to string for output to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inter a single txt fileI，output boundaries encoded in boolean vectors\n",
    "def generate_topic_seg(topic_txt_file):\n",
    "    f = open(topic_txt_file, 'r')\n",
    "    line = f.readline()\n",
    "    count = 0\n",
    "    boundaries = []\n",
    "    # Every segments inside a looping txt file\n",
    "    while line:\n",
    "        # Asterisk does not count in the number of rows\n",
    "        if line.strip() != \"**********\":\n",
    "            count += 1\n",
    "        else:\n",
    "            # When an asterisk is encountered, it indicates that the topic's delimitation occurs,\n",
    "            # and the delimiter points are recorded in a list\n",
    "            boundaries.append(count)\n",
    "        line = f.readline()\n",
    "    # Create a list of 0 elements with a length equal to the number of rows\n",
    "    zero_list = [0] * count\n",
    "    # The value of the element at the separation position is 1\n",
    "    for i in boundaries:\n",
    "        zero_list[i-1] = 1\n",
    "    # Binary vector connected with commas\n",
    "    topic_seg = \",\".join(map(str, zero_list))\n",
    "    meeting_transcript = split(topic_txt_file)[-1].replace(\".txt\", \"\")\n",
    "    # Add file ID, output to file\n",
    "    return \"%s:%s\" % (meeting_transcript, topic_seg)\n",
    "\n",
    "# Loop through each file to get all the vectors\n",
    "def batch_generate_topic_seg(onlyfiles):\n",
    "    topic_seg_list = []\n",
    "    for txt_file in onlyfiles:\n",
    "        topic_seg_list.append(generate_topic_seg(txt_file))\n",
    "    return topic_seg_list\n",
    "\n",
    "# Save each file's corresponding vector into the topic_segs.txt file\n",
    "def output_topic_seg(topic_seg_list, output_file):\n",
    "    f = open(output_file, 'w')\n",
    "    for topic_seg in topic_seg_list:\n",
    "        f.write(\"%s\\n\" %topic_seg)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_seg_output_file = './topic_segs.txt'\n",
    "topic_seg_list = batch_generate_topic_seg(onlyfiles)\n",
    "output_topic_seg(topic_seg_list, topic_seg_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform paragraphs to sparse representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each txt file generated by looping task1    \n",
    "1) Divide paragraphs on the content of txt files  \n",
    "2) Segmentation of paragraphs   \n",
    "3) Count the frequency of occurrence of each word in the paragraph and generate <word:word frequency> as <indicate:value>  \n",
    "4) All of the <indicated:values> in paragraph are connected by a comma and output to the result file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inter a txt file and the word dictionary generated in the first step，generates a sparse vector for each paragraph in the txt file\n",
    "# indicate as word index,  value as the word frequency in the paragraph appear\n",
    "def sparse_txt_file(txt_file, vocab_dict):\n",
    "    f = open(txt_file, 'r')\n",
    "    # Fragment file content by asterisk separator\n",
    "    line = f.readline()\n",
    "    lines = []\n",
    "    while line:\n",
    "        lines.append(line.strip())\n",
    "        line = f.readline()\n",
    "    lines.pop(-1)\n",
    "    paragraph_list = \" \".join(lines).strip().split(\"**********\")\n",
    "    # Segment each paragraph and remove stop words\n",
    "    paragraph_list_tokens = map(lambda x: extract_tokens(x, stopwords), paragraph_list)\n",
    "    # Convert the word to get a list with the form [ indicate:value, ...]\n",
    "    paragraph_sparse_rep = map(lambda x: trans_tokens_to_index(x, vocab_dict), paragraph_list_tokens)\n",
    "    return paragraph_sparse_rep\n",
    "\n",
    "# Use the word dictionary to transform the word into index, and at the same time, count the frequency of \n",
    "# occurrence of each word in the corresponding paragraph.\n",
    "def trans_tokens_to_index(tokens_list, vocab_dict):\n",
    "    # Convert only words that exist in the word dictionary\n",
    "    tokens_index_list = [vocab_dict.get(token) for token in tokens_list if token in vocab_dict]\n",
    "    # How often each word appears in the paragraph\n",
    "    counts = Counter(tokens_index_list)\n",
    "    # word as indicate, freq as value, generates a list of elements named:value\n",
    "    sparse_vec = [\"%s:%s\" % (token, freq) for token, freq in counts.items()]\n",
    "    return \",\".join(sparse_vec)\n",
    "\n",
    "# Loops each paragraph's corresponding sparse vector into the result file\n",
    "def output_sparse_txt(paragraph_sparse_rep, output_file):\n",
    "    f = open(output_file, 'w')\n",
    "    for sparse_rep in paragraph_sparse_rep:\n",
    "        f.write(\"%s\\n\" % sparse_rep)\n",
    "    f.close()\n",
    "\n",
    "def batch_output_sparse_txt(txt_file_list, token_dict, output_dir):\n",
    "    # Loop through each txt file\n",
    "    for topic_txt_file in  txt_file_list:\n",
    "        # Output file name\n",
    "        sparse_rep_output_file = join(output_dir, split(topic_txt_file)[-1])\n",
    "        # Call sparse_txt_file to extract sparse vector of a txt file\n",
    "        paragraph_sparse_rep = sparse_txt_file(topic_txt_file, token_dict)\n",
    "        # Output sparse vector into result file\n",
    "        output_sparse_txt(paragraph_sparse_rep, sparse_rep_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_rep_output_dir = './sparse_files'\n",
    "batch_output_sparse_txt(onlyfiles, token_dict , sparse_rep_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
