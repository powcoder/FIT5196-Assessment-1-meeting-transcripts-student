{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Generate Sparse Representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from os import listdir,makedirs\n",
    "from os.path import isfile, join, split, exists, splitext\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords into list\n",
    "stopwords_file = './stopwords_en.txt'\n",
    "stopwords = [word for word in open(stopwords_file).read().split(\"\\r\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expression to make word tokenization\n",
    "def extract_tokens(document, stopwords):\n",
    "    words = re.findall(\"\\w+(?:[-']\\w+)?\", document.lower())\n",
    "    return [word for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meeting transcripts generated in task 1 \n",
    "def load_txt(txt_file):\n",
    "    f = open(txt_file)\n",
    "    line = f.readline()\n",
    "    sentence_list = []\n",
    "    while line: \n",
    "        if line.strip() != '**********':\n",
    "            sentence_list.append(line.strip())\n",
    "        line = f.readline()\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch load meeting transcripts generated in task 1 \n",
    "def batch_load_txt(onlyfiles):\n",
    "    meeting_transcript_list = []\n",
    "    for txt_file in onlyfiles: \n",
    "        meeting_transcript = load_txt(txt_file)\n",
    "        meeting_transcript_list.extend(meeting_transcript)\n",
    "    return meeting_transcript_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate words dictioinary, key is word, value is index\n",
    "def generate_token_dict(tokens_list):\n",
    "    tokens_set = []\n",
    "    [tokens_set.extend(tokens) for tokens in tokens_list]\n",
    "    distinct_sorted_token = sorted(set(tokens_set))\n",
    "    token_idx = range(0, len(distinct_sorted_token))\n",
    "    sorted_token_set = zip(distinct_sorted_token, token_idx)\n",
    "    token_dict = {}\n",
    "    for word, idx in sorted_token_set:\n",
    "        token_dict[word] = idx\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tokenization for each meeting transcript\n",
    "def generate_segment_tokens_list(meeting_transcript_list):\n",
    "    meeting_transcript_list_tokens = map(lambda x: extract_tokens(x, stopwords), meeting_transcript_list)\n",
    "    frequency = defaultdict(int)\n",
    "    for meeting_transcript_tokens in meeting_transcript_list_tokens:\n",
    "        for token in meeting_transcript_tokens:\n",
    "            frequency[token] += 1\n",
    "    tokens_list = [[token for token in meeting_transcript_tokens if frequency[word] <= 1] for meeting_transcript_tokens in meeting_transcript_list_tokens]\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use words dictionary to vocab.txt\n",
    "def output_vocab(vocab_dict, output_file):\n",
    "    vocab_list = [(word, idx) for word, idx in vocab_dict.items()]\n",
    "    vocab_list_sorted = sorted(vocab_list, key=lambda x:x[0])\n",
    "    f = open(output_file, 'w')\n",
    "    for word, idx in vocab_list_sorted:\n",
    "        f.write(\"%s:%s\\n\" % (word,idx))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use meeting transcripts generated in task 1  to create topic_seg\n",
    "def generate_topic_seg(topic_txt_file):\n",
    "    f = open(topic_txt_file, 'r')\n",
    "    line = f.readline()\n",
    "    count = 0\n",
    "    boundaries = []\n",
    "    while line:\n",
    "        if line.strip() != \"**********\":\n",
    "            count += 1\n",
    "        else:\n",
    "            boundaries.append(count)\n",
    "        line = f.readline()\n",
    "    zero_list = [0] * count\n",
    "    for i in boundaries:\n",
    "        zero_list[i-1] = 1\n",
    "    topic_seg = \",\".join(map(str, zero_list))\n",
    "    meeting_transcript = split(topic_txt_file)[-1].replace(\".txt\", \"\")\n",
    "    return \"%s:%s\" % (meeting_transcript, topic_seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate topic_seg for multiple txt file\n",
    "def batch_generate_topic_seg(onlyfiles):\n",
    "    topic_seg_list = []\n",
    "    for txt_file in onlyfiles:\n",
    "        topic_seg_list.append(generate_topic_seg(txt_file))\n",
    "    return topic_seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output topic-seg generated by multiple txt file to tog_seg.txt\n",
    "def output_topic_seg(topic_seg_list, output_file):\n",
    "    f = open(output_file, 'w')\n",
    "    for topic_seg in topic_seg_list:\n",
    "        f.write(\"%s\\n\" %topic_seg)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_tokens_to_index(tokens_list, vocab_dict):\n",
    "    tokens_index_list = [vocab_dict.get(token) for token in tokens_list if vocab_dict.has_key(token)]\n",
    "    counts = Counter(tokens_index_list)\n",
    "    sparse_vec = [\"%s:%s\" % (token, freq) for token, freq in counts.items()]\n",
    "    return \",\".join(sparse_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use word dictionary to transform word to index, then count the frequency of the word, concat with ','\n",
    "def sparse_txt_file(txt_file, vocab_dict):\n",
    "    f = open(txt_file, 'r')\n",
    "    line = f.readline()\n",
    "    lines = []\n",
    "    while line:\n",
    "        lines.append(line.strip())\n",
    "        line = f.readline()\n",
    "    lines.pop(-1)\n",
    "    paragraph_list = \" \".join(lines).strip().split(\"**********\")\n",
    "    paragraph_list_tokens = map(lambda x: extract_tokens(x, stopwords), paragraph_list)\n",
    "    paragraph_sparse_rep = map(lambda x: trans_tokens_to_index(x, vocab_dict), paragraph_list_tokens)\n",
    "    return paragraph_sparse_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sparse representations to a txt file\n",
    "def output_sparse_txt(paragraph_sparse_rep, output_file):\n",
    "    f = open(output_file, 'w')\n",
    "    for sparse_rep in paragraph_sparse_rep:\n",
    "        f.write(\"%s\\n\" % sparse_rep)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write batch sparse representations to a txt file\n",
    "def batch_output_sparse_txt(txt_file_list, token_dict, output_dir):\n",
    "    for topic_txt_file in  txt_file_list:\n",
    "        sparse_rep_output_file = join(output_dir, split(topic_txt_file)[-1])\n",
    "        paragraph_sparse_rep = sparse_txt_file(topic_txt_file, token_dict)\n",
    "        output_sparse_txt(paragraph_sparse_rep, sparse_rep_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "txt_files_dir = './txt_files'\n",
    "onlyfiles = [join(txt_files_dir, f) for f in listdir(txt_files_dir) if (isfile(join(txt_files_dir, f))) and (splitext(f)[1] == '.txt')]\n",
    "\n",
    "meeting_transcript_list = batch_load_txt(onlyfiles)\n",
    "segment_tokens_list = generate_segment_tokens_list(meeting_transcript_list)\n",
    "token_dict = generate_token_dict(segment_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word_string:integer_index to vocab.txt\n",
    "vocab_output_file = './vocab.txt'\n",
    "output_vocab(token_dict, vocab_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the topic boundaries encoded in boolean vectors to topic_segs.txt \n",
    "topic_seg_output_file = './topic_segs.txt'\n",
    "topic_seg_list = batch_generate_topic_seg(onlyfiles)\n",
    "output_topic_seg(topic_seg_list, topic_seg_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sparse representations for all its paragraphs to .txt in the \"sparse_files\" folder\n",
    "sparse_rep_output_dir = './sparse_files'\n",
    "batch_output_sparse_txt(onlyfiles, token_dict , sparse_rep_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
