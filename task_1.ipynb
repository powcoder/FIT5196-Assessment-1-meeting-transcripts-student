{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name:\n",
    "#### Student ID: \n",
    "\n",
    "Date: 02/04/2017\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* xml.etree.ElementTree (for parsing XML doc)\n",
    "* pandas 0.20.2 (for cut function) \n",
    "* re 2.2.1 (for regular expression) \n",
    "* os (for join path, split file name, check the file if exists)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "meeting transcripts are stored in three different types of XML files, which are ending with \".words.xml\", \".topic.xml\" and \".segments.xml\". The task here is to reconstruct the original meeting transcripts with the corresponding topical and paragraph boundaries from these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "from os import listdir,makedirs\n",
    "from os.path import isfile, join, split, exists, splitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defines constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program first defines some of the constants that will be used later in the process.   \n",
    "TOPIC_TAG will be used to identify which element is the node that contains the topic.  \n",
    "SEGMENTS_TAG will be used to identify that element is a node containing segments.     \n",
    "WORD_XML_FILE_DIR is the directory where the .word.xml file is stored.  \n",
    "TOPIC_XML_FILE_DIR is the directory where the .topic.xml file is stored.   \n",
    "SEGMENTS_XML_FILE_DIR is the directory where the .segments.xml file is stored.  \n",
    "TXT_OUTPUT_DIR is a directory for storing the data result of task1.  \n",
    "BRAKETS_CONTENS_PATTERN is a regular expression used to extract the contents of parentheses.  \n",
    "WORD_ID_PATTERN is a regular expression used to extract the word id from a string.  \n",
    "WORD_ID_PATTERN_X is a regular expression used to extract the word id from a string, and the difference between WORD_ID_PATTERN is whether the prefix of id has an x.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOPIC_TAG = \"{http://nite.sourceforge.net/}id\"\n",
    "SEGMENTS_TAG = \"href\"\n",
    "WORD_XML_FILE_DIR = './words'\n",
    "TOPIC_XML_FILE_DIR = './topics'\n",
    "SEGMENTS_XML_FILE_DIR = './segments'\n",
    "TXT_OUTPUT_DIR='./txt_files'\n",
    "BRAKETS_CONTENS_PATTERN = '\\((.*?)\\)'\n",
    "WORD_ID_PATTERN = r'words([0-9]+)'\n",
    "WORD_ID_PATTERN_X = r'wordsx([0-9]+)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Resolve a single .topic.xml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the root node of the xml file is obtained, and each layer under the root node is processed for processing. The processing here includes:\n",
    "1)Retrieve the value of the <topic nite:id> tag of the root node of this layer as a topic.\n",
    "2)Iterate over each element below the layer, skip the element of the <nite:pointer> role tag, and from the inside of the element containing the <nite:child href> tag, extract the vocabulary beginning ID, vocabulary end ID, and corresponding one representing a segment. The word.xml file name, for elements containing the <topic nite:id> tag, repeats step 2) for recursive traversal processing until all data is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input is a topic.xml file\n",
    "def parse_topic_xml(topic_xml):\n",
    "    tree = ET.parse(topic_xml) \n",
    "    #Get root node\n",
    "    root = tree.getroot() \n",
    "    parse_list = [] #Create a list for storing the parsed topic, vocabulary file name, vocabulary start ID, vocabulary end ID\n",
    "    for child in root: #Traversing the next level of the root node\n",
    "        topic = child.attrib.get(TOPIC_TAG) # Get the topic value from the topic tag\n",
    "        for sub in child: # Traverse every element below the layer\n",
    "            # When the attribute of the element contains the <nite:pointer> role tag, \n",
    "            # the description does not include the segment to be extracted, skipping directly\n",
    "            if \"role\" in sub.attrib: \n",
    "                pass\n",
    "            # If the <topic nite:id> tag appears, there is a subtopic under this layer\n",
    "            elif sub.attrib.get(TOPIC_TAG): \n",
    "                # Also need to skip the element that contains the <nite:pointer> role tag\n",
    "                if not \"role\" in sub[0].attrib: \n",
    "                    # Recursive calls parse subtopics and add data to parse_list\n",
    "                    parse_sub(sub, parse_list, topic) \n",
    "            else: \n",
    "                # When the <nite:child> tag appears, it means that below the element, the segment corresponding to the topic can be obtained.\n",
    "                # Get tag nite:child href value\n",
    "                info_str = sub.attrib.get(SEGMENTS_TAG) \n",
    "                # The calling function extracts the vocabulary file name from the info_str, the vocabulary start ID, the vocabulary end ID\n",
    "                (word_xml_file, word_start_id, word_end_id) = extracte_word_file_and_id(info_str) \n",
    "                # Add the three fields extracted from info_str with topic to the parse_list\n",
    "                parse_list.append((topic, word_xml_file, word_start_id, word_end_id)) \n",
    "    return parse_list\n",
    "\n",
    "#In order to explore the tree structure of an XML node, a small recursive routine is defined to get the tag names of all descendants of any given \n",
    "#node. \n",
    "# There are two inputs, one is the sub topic node content under root topic, and the other is root topic\n",
    "def parse_sub(sub, parse_list, belong_topic):\n",
    "    for sub_sub in sub: \n",
    "        # Traversing each element in the sub\n",
    "        sub_topic = sub.attrib.get(TOPIC_TAG) \n",
    "        # Does not handle elements containing <nite:pointer> role tags\n",
    "        if not \"role\" in sub_sub.attrib: \n",
    "            # Handle the data that contains the segment\n",
    "            if not TOPIC_TAG in sub_sub.attrib: \n",
    "                # Call function to extract vocabulary file name from info_str, vocabulary start ID, vocabulary end ID\n",
    "                (word_xml_file, word_start_id, word_end_id) = extracte_word_file_and_id(sub_sub.attrib.get(SEGMENTS_TAG))\n",
    "                # Save the parsed field data\n",
    "                parse_list.append((belong_topic, word_xml_file, word_start_id, word_end_id)) \n",
    "            else:\n",
    "                # When the sub topic still has children, call again for parsing\n",
    "                parse_sub(sub_sub, parse_list, belong_topic) \n",
    "\n",
    "# An input, <nite:child href> tag value\n",
    "def extracte_word_file_and_id(attrib_str):\n",
    "    sub_attrib_split = attrib_str.split(\"#\")\n",
    "    word_xml_file = sub_attrib_split[0] # Extract the file name of .word.xml\n",
    "    word_ids = re.findall(BRAKETS_CONTENS_PATTERN, sub_attrib_split[1])\n",
    "    # Extract word_start_id and word_end_id, when there is only one word, word_start_id is equal to word_end_id\n",
    "    condition = len(word_ids) > 1\n",
    "    (word_start_str, word_end_str) = (word_ids[0], word_ids[1]) if condition else (word_ids[0], word_ids[0])\n",
    "    word_start_id = re.findall(WORD_ID_PATTERN, word_start_str)[0]\n",
    "    word_end_id = re.findall(WORD_ID_PATTERN, word_end_str)[0]\n",
    "    # Return the extraction result\n",
    "    return word_xml_file, word_start_id, word_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', '0', '43')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.C.words.xml', '0', '0')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', '0', '4')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.C.words.xml', '1', '2')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', '5', '6')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', '44', '64')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', '7', '7')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', '65', '107')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', '8', '8')\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', '108', '165')\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "topic_xml_file = \"./topics/TS3009c.topic.xml\"\n",
    "parse_element_list = parse_topic_xml(topic_xml_file)\n",
    "topic_xml_filename_prefix = split(topic_xml_file)[1].replace(\".topic.xml\", \"\")\n",
    "sample = 10\n",
    "for parse_element in parse_element_list[:sample]:\n",
    "    print(parse_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reconstructed with the \"*.segments.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each segment extracted from the tag href, read paragraph delimitation in \".segments.xml\", Adjust the word_start_id and word_end_id in the segment. For example, for word0..word30, in the \"segment.xml\" file, the correct delimitation is word0..word10, word11..word30, and word0..word30 is divided into two segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the .segments.xml file to correct paragraphs\n",
    "def adjust_paragraph_boundary(parse_element_list):\n",
    "    # Create a list to save the result of adjusting the paragraph delimitation\n",
    "    adjust_parse_element_list = [] \n",
    "    for topic, word_xml_file, word_start_id, word_end_id in parse_element_list:\n",
    "        #Get the corresponding \"segments.xml\" file\n",
    "        segments_xml_file =  word_xml_file.replace(\"words\", \"segments\") \n",
    "        # Passing segments_xml to get the last paragraph of the paragraph demarcation\n",
    "        paragraph_up_boundaries = load_paragraph_boundary(segments_xml_file) \n",
    "        paragraph_boundaries = cut_segment_with_paragraph_boundary(int(word_start_id), int(word_end_id), paragraph_up_boundaries)\n",
    "        adjust_parse_element_list.append((topic, word_xml_file, paragraph_boundaries))\n",
    "    return adjust_parse_element_list\n",
    "\n",
    "# Reading the upper bound of the paragraph delimitation from the segments.xml file\n",
    "def load_paragraph_boundary(segments_xml_file):\n",
    "    segments_xml_file_path = join(SEGMENTS_XML_FILE_DIR, segments_xml_file)\n",
    "    tree = ET.parse(segments_xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    # List for storing demarcation points\n",
    "    paragraph_boundaries_bins = [] \n",
    "    # Parse each layer under the root node of the segments_xml file\n",
    "    for child in root:\n",
    "        for sub in child:\n",
    "            #Parse the vocabulary file name from the content of the <nite:child href> tag, the vocabulary start ID, the vocabulary end ID\n",
    "            (word_xml_file, word_start_id, word_end_id) = extracte_word_file_and_id(sub.attrib.get(SEGMENTS_TAG))\n",
    "            #Save the upper bound of each paragraph\n",
    "            paragraph_boundaries_bins.append(int(word_end_id)) \n",
    "    return paragraph_boundaries_bins \n",
    "\n",
    "def cut_segment_with_paragraph_boundary(word_start_id, word_end_id, paragraph_up_boundaries):\n",
    "    temp_paragraph_boundaries = []\n",
    "    # Used to store results\n",
    "    adjust_paragraph_boundaries = [] \n",
    "    # Save the boundary in the range (word_start_id, word_end_id) to a new list\n",
    "    [temp_paragraph_boundaries.append(up_bound) for up_bound in paragraph_up_boundaries if up_bound > word_start_id and up_bound < word_end_id]\n",
    "    # Insert word_start_id at the head of the list, plus word_end_id at the end\n",
    "    # Generate an interval that can be used to segment the word sequence word_start_id:word_end_id\n",
    "    temp_paragraph_boundaries.insert(0, word_start_id)\n",
    "    temp_paragraph_boundaries.append(word_end_id)\n",
    "    interval_list = []\n",
    "    # Dividing the sequence of words id by paragraph demarcation\n",
    "    if len(set(temp_paragraph_boundaries)) == 1:\n",
    "        #  If a word is done alone\n",
    "        interval_list = [(temp_paragraph_boundaries[0], temp_paragraph_boundaries[0])]\n",
    "    else:\n",
    "        cut_result = pd.cut(range(word_start_id, word_end_id + 1), temp_paragraph_boundaries)\n",
    "        # Handle the segmentation results, generate tuples, add to the result list\n",
    "        for i in cut_result.categories:\n",
    "            interval_str = i.replace(\"(\", \"\").replace(\"]\", \"\").replace(\" \",\"\").split(\",\")\n",
    "            interval_list.append((int(interval_str[0]), int(interval_str[1])))\n",
    "    for idx, interval in enumerate(interval_list):\n",
    "        if idx == 0:\n",
    "            adjust_paragraph_boundaries.append((interval[0], interval[1]))\n",
    "        else:\n",
    "            # The lower bound is +1 because the segmentation result interval is left open and right, but the first interval does not need to be processed.\n",
    "            adjust_paragraph_boundaries.append((interval[0] + 1, interval[1]))\n",
    "    return adjust_paragraph_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', [(0, 43)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.C.words.xml', [(0, 0)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', [(0, 2), (3, 4)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.C.words.xml', [(1, 2)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', [(5, 6)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', [(44, 45), (46, 64)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', [(7, 7)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', [(65, 107)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.D.words.xml', [(8, 8)])\n",
      "('TS3009c.topic.vkaraisk.2', 'TS3009c.A.words.xml', [(108, 165)])\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "adjust_parse_element_list = adjust_paragraph_boundary(parse_element_list)\n",
    "for adjust_parse_element in  adjust_parse_element_list[:sample]:\n",
    "    print(adjust_parse_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Match the \"*.word.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the word.xml file and match the word based on the id. For example:  \n",
    "Segments parsed from topic are:  \n",
    "topic1 word0..word3  \n",
    "topic2 word4..word5  \n",
    "In the corresponding .word.xml file:  \n",
    "word0: \"this\"\n",
    "word1: \"is\"\n",
    "word2: \"the\"\n",
    "word3: \"kick-off\"\n",
    "word4: \"meeting\"\n",
    "Will be generated:  \n",
    "topic1 \"this is the\"\n",
    "topic2 \"kick meeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_the_word_xml(adjust_parse_element_list, topic_xml_filename_prefix):\n",
    "    #Used to save data after matching word\n",
    "    paragraph_contents_data = [] \n",
    "    # The word content dictionary is read from the word.xml file with the prefix topic_xml_filename_prefix\n",
    "    word_dict_with_id = load_word_dict_with_id(topic_xml_filename_prefix) \n",
    "    # Iterate through the adjust parsed results of paragraph boundaries\n",
    "    for topic, word_xml_file, paragraph_boundaries in adjust_parse_element_list:  \n",
    "        word_list_with_id = word_dict_with_id[word_xml_file]\n",
    "        # According to the division interval, the words in the interval are taken out into the word dictionary, \n",
    "        # and the words in the interval are connected with spaces.\n",
    "        for paragraph_start, paragraph_end in paragraph_boundaries:\n",
    "            wordid_word_tuple_list = get_word(word_list_with_id, int(paragraph_start), int(paragraph_end))\n",
    "            paragraph = \" \".join([word for word_id,word in wordid_word_tuple_list])\n",
    "            # Add the word join result and topic together to the result list, remove the empty\n",
    "            if len(paragraph) > 0:\n",
    "                paragraph_contents_data.append((topic, paragraph))\n",
    "    return paragraph_contents_data\n",
    "\n",
    "# Given a prefix, match the corresponding files from word.xml and read the files to build a word dictionary\n",
    "# The format is {word.xml file name: [(word_id, word)]}\n",
    "def load_word_dict_with_id(prefix):\n",
    "    # Match files based on the prefix\n",
    "    onlyfiles = [f for f in listdir(WORD_XML_FILE_DIR) if (f.startswith(prefix)) & (isfile(join(WORD_XML_FILE_DIR, f)))]\n",
    "    # Create an empty dict \n",
    "    word_id_dict = {}\n",
    "    #Traversing a file that satisfies the conditions\n",
    "    for word_xml_file in onlyfiles:\n",
    "        word_list_with_id = []\n",
    "        word_tree = ET.parse(join(WORD_XML_FILE_DIR, word_xml_file))\n",
    "        word_root = word_tree.getroot()\n",
    "        # Traverse each layer under the root node of the \"word.xml\" file and extract the word corresponding to each word_id.\n",
    "        for child in word_root:\n",
    "            word_key = child.attrib.get(TOPIC_TAG)\n",
    "            word_key_id_list = re.findall(WORD_ID_PATTERN, word_key)\n",
    "            if len(word_key_id_list) > 0:\n",
    "                word_key_id = word_key_id_list[0]\n",
    "            else:\n",
    "                word_key_id = re.findall(WORD_ID_PATTERN_X, word_key)[0]\n",
    "            word = child.text\n",
    "            word_list_with_id.append((int(word_key_id), word))\n",
    "        # Put the word taken from the \"word.xml\" file in a list as the value of the dict\n",
    "        word_id_dict[word_xml_file] = word_list_with_id\n",
    "    return word_id_dict\n",
    "\n",
    "# Determine if the id corresponding to a word is within the interval [word_start_id, word_end_id]\n",
    "def filter_fun(tuple2, word_start_id, word_end_id):\n",
    "    word_id, word = tuple2[0], tuple2[1]\n",
    "    return word_id >= word_start_id and word_id <= word_end_id and word is not None\n",
    "# Filter all words in the range [word_start_id, word_end_id]\n",
    "def get_word(word_list_with_id, word_start_id, word_end_id):\n",
    "    return filter(lambda x: filter_fun(x, word_start_id, word_end_id), word_list_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TS3009c.topic.vkaraisk.2', \"Okay . Uh door is closed . Well , let's begin . Because if we have as much time as the last uh meeting , we'll have to hurry up . Um well I'll start with the presentation again , the agenda .\")\n",
      "('TS3009c.topic.vkaraisk.2', \"I'm listening .\")\n",
      "('TS3009c.topic.vkaraisk.2', 'Right .')\n",
      "('TS3009c.topic.vkaraisk.2', 'Great .')\n",
      "('TS3009c.topic.vkaraisk.2', 'Yo .')\n",
      "('TS3009c.topic.vkaraisk.2', 'So . Uh This one I think . Uh yeah . Well alright .')\n",
      "('TS3009c.topic.vkaraisk.2', \"Um well , I'll show you the notes . It's not as uh interesting as it should be because we just uh had the meeting , but I'll show them . We'll get your presentations again on the conceptual design . Um\")\n",
      "('TS3009c.topic.vkaraisk.2', \"Then we'll have to dec decide about the control , the remote control concepts . I've put a f uh a file in the project management folder , which says exactly uh what kind of decisions we should take . So this time we exactly know what to decide about . And then we'll close again .\")\n",
      "('TS3009c.topic.vkaraisk.2', 'Mm-hmm .')\n",
      "('TS3009c.topic.vkaraisk.2', 'Alright , great .')\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "match_result_list = match_the_word_xml(adjust_parse_element_list, topic_xml_filename_prefix)\n",
    "for match_result in match_result_list[:sample]:\n",
    "    print( match_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Format output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loop through each topic.xml file.  \n",
    "1)Call a function that parses a single topic.xml file.  \n",
    "2)For the parsed result of 1), read segment.xml to reconstruct the paragraph.   \n",
    "3)For the reconstruction result of 2), read the words.xml file to match and extract the contents of Word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For segments that have been parsed, corrected, and matched with good words, formatted and output to txt file\n",
    "def format_output(match_result_list, output_file):\n",
    "    f = open(output_file, 'w')\n",
    "    prev_topic = match_result_list[0][0]\n",
    "    for topic, paragraph in match_result_list:\n",
    "        if topic != prev_topic:\n",
    "            f.write(\"**********\" + \"\\n\")\n",
    "        f.write(\" %s\\n\" % paragraph)\n",
    "        prev_topic = topic\n",
    "    f.write(\"**********\" + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "# Parse files in batches, correct paragraphs, match texts, and save to txt files\n",
    "def batch_parse_topic_xml(output_dir):\n",
    "    onlyfiles = [join(TOPIC_XML_FILE_DIR, f) for f in listdir(TOPIC_XML_FILE_DIR) if (isfile(join(TOPIC_XML_FILE_DIR, f))) and (splitext(f)[1] == '.xml')]\n",
    "    file_size = len(onlyfiles)\n",
    "    for i in range(file_size):\n",
    "        # The \"topic.xml\" file that needs to be parsed\n",
    "        topic_xml_file = onlyfiles[i]\n",
    "        # Generate output file path\n",
    "        topic_prefix =  split(topic_xml_file)[1].replace(\".topic.xml\", \"\")\n",
    "        output_file = join(output_dir, \"%s.txt\" % topic_prefix)\n",
    "        # Parse the topic file\n",
    "        parse_element_list = parse_topic_xml(topic_xml_file)\n",
    "        # Correct paragraph demarcation\n",
    "        adjust_parse_element_list = adjust_paragraph_boundary(parse_element_list)\n",
    "        # Match vocabulary content\n",
    "        match_result_list = match_the_word_xml(adjust_parse_element_list, topic_prefix)\n",
    "        # Output to file\n",
    "        format_output(match_result_list, output_file)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_parse_topic_xml(TXT_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
